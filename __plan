# immediately
1. <s>generate market volatility data
    1. <s>grab daily data of s&p500 from yahoo
    2. <s>calculate volatility for each day for s&p500
        - prior, future, center
2. <s>inspect market returns -vs- volatility
3. <s> retrieve news data and normalize the data
    - use the dataset from [37], [38]: https://github.com/philipperemy/financial-news-dataset
    - normalize:
        1. load all data into pandas
        2. cast "date" field into readable date
        3. extract "catagory" from url of reuters news
0. <s> explore the news data
    4. <s>assess frequency of news / day
0. <s> assess news occurrence -vs- volatility
    2. see if article frequency correlates with volatility by graphing
0. <s> label market periods as "high", "moderate", and "low" volatility
0. <s> assess vol_day text -vs- volatility - basic feature representation - token level representations - very naive
    - combine text found in past X days of a volatility (a vol_day) and see if it corresponds to volatility
        - e.g., the words "drop" and "crash" are probably related to more volatile days. see if we can find this from a naive search
    0. <s> tokenize each title of each news report
        - include date + tokens list
        - generates : `news/...tokens.csv`
    0. <s> generate list of all tokens found in past X days of news for each volatility day
        - generates: `combined/tokens_for_each_vol...`
        - e.g., past 2, 5, 10 days of news (x=[2, 5, 10, 15])
        1. <s> for each volatility, get past X days of news of that volatility day, and generate list of all tokens from news for that period
            - don't include the volatility day, since we want to predict based on previous data
    0. <s> assess normalized term frequency of words -vs- labels
        - use `combined/tokens_for_each_vol` data
        - assess term frequency differences between:
            - low and high over all years
            - high volatility across several years
            - low volatility across several years
            - low and high over several years
        1. generate full list of tokens for each class in range
        2. evaluate differences between classes in range
    - **RESULT:** no clear relationship between simply the words seen in the past 2 days and the future 5 day volatility
        - different combinations of past x days and past 5 days not explored becuase it does not seem very likely (nor interesting)
            - it is more likely that the real information is stored when words are used in context, in a document by document representation or even sequential document representations
0. assess vol_day text -vs- volatility - TF-IDF representation
    0. <s> calculate tf-idf of each `combined/tokens_for_each_vol...`
        - generates `combined/tokens_for_each_vol.tf-idf.csv`
        - calculates tf-idf using each vol_day as a document
    0. assess vol_based tf-idf of ranges+labels
        - cluster the TF-IDF vectors w/ kmeans and see if the classes divide well
        - classify the TF-IDF vectors and see if performance is good
    *** RESULT:***
        - 64 gigs not enough memory to generate TFIDF-SVD min3 vectors  for 2007-2018
        - clustering (evaluated w/ 2014-2018 tfidf-svd min3 vectors):
            - 6 clusters -> avg ~51% purity (when normalized to reflect HIGH FOUND / ALL HIGH)
            - 15 clusters -> avg ~65% purity (but still the ABS purity of high in the best cluster is very low!)
            - summary: highs are scattered across all classes. even if high is max purity normalized (e.g., 24% of highs are in a class and surpass 14% of lows - its still ~14 highs compared to 500 lows)
                - NO GOOD SEPARATION
        - classification:
            - RF for 2014-2018 :
                - when setting classweight of HIGH to 100 (compared to 1 and 1 for low and med) recall is great (some of the time for test...). precision is bad though in both training and testing. (~20%)
                    - good precision for test may be comming from overfitting train data: if overfit then when we see another datapoint from similar time period it will likely classify it correctly since news overlapps
                        - ***TODO*** change test/train split so that test is in future of split - not randomly distributed
            - RF for 2007-2018
                - 1. update calculation of TFIDF-SVD min3 to only have dimensionality of 300 and see if that enables us to gen the labels
                    - if that doesn't work, try with min5 
0. assess documents -vs- volatility - document level representation
    - more reflective of where real value is (some documents do affect vol and some do not)
    - consider representations of documents that may capture data relevant to predicting volatility
        - tf-idf of each document before merging into `vol_based.period` datas
        - basic word vector combinations
            - averaging
            - averaging nouns
        - SEARCH LITERATURE
    - assess promising representations of data relevant to predicing period statistically
        - promising representations are those generated from above task
        - consider clustering each document and then seeing whether clusters contain good separation of classes
        - other assessment methods
    0. TF-IDF representation
        0. represent documents as TF-IDF vectors
        0. assess TF-IDF vector documents w/ clustering
    0.
0. assess vol_day documents -vs- volatility - combination of documents into representation for past vol_day relevant news
    - e.g., combine document level representations
    - consider using sequential occurrence of documents ?
        - e.g., RNN
0. generate baseline representations of news (see `ideas/text_rep...` for all)
    - bag of words (all, title, content)
    - tf-idf (all, title, content)
0. generate prediction pipeline on baseline representations
    - svm
        - used by [43]
    - recurrent neural network
0. assess baseline representations
0. assess autoencoder representations of news

# eventually
- assess news occurence -vs- volatility for a particular stock
- assess "event" representation of news
